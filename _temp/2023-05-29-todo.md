"·
1. 캐릭터당 draw call 개수 (https://blog.naver.com/hooki3/221035417113)
- 재질에 따라 드로우하니 재질별로 머리,얼굴,그외로 나누면 되지만
- 같은 머터리얼이지만 텍스처가 다른 몸과 손은 따로 그리고 있음. 
- srp batch를 사용하면 하나로 그릴 수 있나?
- 그림자 (4 cascade) + 프리패스 + 파츠별 * 라이트 개수 (메인 라이트 + additional) + water (reflection)
2. signed distance (https://scahp.tistory.com/17)
3. kd 알고 (https://www.baeldung.com/cs/k-d-trees) or r tree
4. str 알고
5. Terrain Layers
6. mono time steady time
7. covolution (확률 함수/적분/계수/확률이산분포)
8. marshal
9. pbr vs none pbr
10. string 과 stream
11. make_shared를 사용하는 이유, 순환 참조
12. bcs class 구조 (어떤 생각으로 짰는지, iocp zero copy)
13. terrain rendering (https://www.reddit.com/r/GraphicsProgramming/comments/13i55ae/whats_the_stateoftheart_terrain_rendering/)
14. array segment (https://velog.io/@yarogono/C-ArraySegment%EB%9E%80) c++에서 같은 기능을 하는 클래스는 없나요? C++에서는 포인터 사용으로 부분 접근이 가능합니다. (배열에서 특정 부분을 따로 때낼수가 없다는 거구나)
15. shadow (https://illu.tistory.com/1253)
16. shadow mask (https://catlikecoding.com/unity/tutorials/custom-srp/shadow-masks/)
17. pixel to world when (https://stackoverflow.com/questions/13419605/how-to-map-x-y-pixel-to-world-cordinates)
18. unity rendering (https://junbastick.tistory.com/6)
19. hdr (https://docs.unity3d.com/560/Documentation/Manual/HDR.html)
20. tone mapping (https://forum.unity.com/threads/texture-creation-workflow-for-tonemapping.1343789/) / 실제로 hdr 에서 tonemap 하기전까지 밝아야하는게 아닌지... 그런 셈풀이 있는지
21. gamma space (https://blog.naver.com/PostView.nhn?blogId=cdw0424&logNo=221827528747)
22. bloom filtering 
- https://junbastick.tistory.com/67?category=1008778
- https://learnopengl.com/code_viewer_gh.php?code=src/5.advanced_lighting/7.bloom/bloom.cpp (MRT)
- unity는 original을 받아서 다운 셈플링하면서 시작 (MRT는 원본과 같은 상태 필터링 후 그걸 인풋으로 시작)
23. FragmentCommonData 이거 확인
24. srgb (https://unlimited3d.wordpress.com/2020/01/08/srgb-color-space-in-opengl/)
25. simd & shader if statement (https://jahej.com/alt/2011_04_06_if-simd-o_o-or-how-to-vectorize-code-with-different-execution-paths-2.html, https://community.khronos.org/t/if-statements-in-shaders-confused-by-results/73518/7, https://www.reddit.com/r/GraphicsProgramming/comments/vrafmt/question_about_gpus_and_simd/)
- https://gamedev.stackexchange.com/questions/62648/what-does-ddx-hlsl-actually-do
- lerp(x, y, step(a, b))
- simd 는 어셈블리로 이해해야겠네.
- 기본적으론 vector이지만 텍스쳐등로 vector를 읽지 않는 이상 다 같은 값이 적용 되지 않을까?
- gpu simd https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/
- elixir 에 for break는 없고 Enum.reduce가 더 많이 쓰이는 이유? reduce_while은 비효율적인 이유?
26. character 조립 확실히
27. vec3(0.2126f, 0.7152f, 0.0722f) Luminance Coefficients
28. resourcebundle이 되어 다른 asset의 폴더를 가져감
- resourcebundle은 몬스터 A, 몬스터 B 같은 조립 정보 그외 asset/character는 실제 리소스
- 모든 리소스는 폴더에 따른 번들 이름을 가짐 
29. c# marshal (https://hwanine.github.io/network/Marshalling/)
30. GGX PBS Schick Fresnel (https://lifeisforu.tistory.com/500; https://blog.naver.com/canny708/221551990444)
31. deep learning (https://www.youtube.com/watch?v=ENMtsWy52WA)
32. ndotl, ndotv, ndoth (https://madumpa.blogspot.com/2019/01/ndotl.html)
33. tagent space
34. assetstudio
35. Ramp Wrap Light (Toony Colors Pro 2, PhotoFilters, https://github.com/GameDevBox/Custom-Lighting-Toon-Ramp)
36. Toony Colors Pro 2(https://chulin28ho.tistory.com/543)
coding3study@gmail.com
37. 곱셈(필터링) 덧셈 밝기 추가
38. vulkan (https://ogldev.org/www/tutorial50/tutorial50.html)
39. surf shader principle (https://docs.unity3d.com/Manual/SL-SurfaceShaderLightingExamples.html)
40. custom lighting in shader graph (https://darkcatgame.tistory.com/89)
- https://unity.com/kr/blog/engine-platform/custom-lighting-in-shader-graph-expanding-your-graphs-in-2019
- https://darkcatgame.tistory.com/89
- https://chulin28ho.tistory.com/559
41. water (https://ameye.dev/notes/stylized-water-shader/; https://github.com/Unity-Technologies/BoatAttack; 
https://www.youtube.com/watch?v=wAC77JcJEnM&list=PL9vAaGGYE8Tp7NtUdpZbTlEjypWLs4H2M&index=3 (2019.3.0f6); https://www.cyanilux.com/tutorials/2d-water-shader-breakdown/; https://www.youtube.com/watch?v=Z_HUntydy8I&t=7s; https://darkcatgame.tistory.com/30)
- reflection (probe reflection or render target?)
(renderType=Opaque,surf probe mission = sample cubemap, light color zero하면 probe 원본을 볼 수 있음)
- refraction 
물 밑을 먼저 그리고 물은 opaque 이지만 queue를 transpancy로 그려서 먼저 그린 framebuffer 의 색상을 읽어서 물 밑을 그린다.
- GL.invertCulling = true (reflection)
- water is unlit? screen space distance? (물은 투명이 아니라 불투명이고 굴절과 반사를 불투명 물에 그리는거네)
42. quaternion (https://www.youtube.com/watch?v=vpPWoCEs-xs&list=PLrrTotxaO6khV32NMle-VYOou5gK52Q_1&index=3)
43. 튜링 머신 (계산 능력)
44. 현재 바인딩 된 프레임 버퍼의 칼라 attachement 텍스쳐는 쉐이더에서 셈플링 불가
- 유니티에선 blit 이고 opengl 로 이전 프레임 버퍼의 칼라 텍스쳐를 액티브 텍스쳐로 지정해 쉐이더에서 읽는건 가능
- UI blur 처리시 grap pass를 쓰고 있는데 pipe line stage 병렬처리가 깨짐
- 더욱이 모든 화면을 전부 copy하는거라 사용하지 않는 부분도 복사하여 비효율적
- two multi target으로 밝은 부분만 따로 출력
45. opengl GLFW library c++ solution type
46. learn c++ (https://www.learncpp.com/)
48. post process (실루엣, 엑스레이)
49. built-in to srp (https://shakddoo.tistory.com/entry/Unity-Built-in-to-URP-%EB%B3%80%ED%99%98%ED%95%98%EA%B8%B0)
50. multi-directional light, proxy light (indirect)
51. anti-aliasing (https://learnopengl.com/Advanced-OpenGL/Anti-Aliasing)
- SSAA 보이는 것보다 크게 그리고 다운 셈플링하면서 지그재그 삭제. 비용큼. 지금은 사용 안함
- MSAA 하드웨어서 픽셀 색상을 받아 이전에 있던 색과 섞을 때 subsample 하여 섞음 (블랜드가 없으면 경계면만 섞음)
- 픽셀당 x2, x4, x8 만큼 subsample을 하여 혼합
- MSAA 매번 픽셀 쉐이더 이후에 실행, FXAA PP로 한번만 실행
- MSAA 가 더 좋은 퀄리티, FXAA는 적은 비용
- MSAA 와 FXAA 둘다 사용하기도 함
52. framebuffer
- os에서 기본 제공하는 graphic api가 컨트롤 못하는 framebuffer를 사용자가 생성하여 사용 가능
- color attachement에 linear 지정, 여러개의 attachement 사용, anti-aliasing 사용 기본에선 할 수 없는 옵션 이용 가능
- 단 anti-aliasing(msaa) 사용시엔 blit으로 anti-aliasing 데이터를 decoding하여 텍스쳐를 새로 만들고 
- 그걸 다시 인풋 텍스쳐로 읽어 glBindFramebuffer(GL_FRAMEBUFFER, 0)를 하여 os 기본 프레임 버퍼에 quad 버텍스로 (화면크기) 다시 그림
53. shader_feature vs multi_compile 
- multi_compile 로 지정 된 경우에 수만큼 쉐이더가 생성됨 (머터리얼 및 전역 세이더 키워드에 따라 런타임에 사용 되는 쉐이더를 선택)
- multi_compile _ FOO_ON 하면 아무것도 없는 것과 FOO_ON 인 두개의 세이더 베리언트를 생성 
- shader_feature 는 사용하지 않으면 게임 빌드에 추가 안되는 유일한 차이가 있음
- 따라서 무조건 포함 되는 multi_compile는 전역적으로 사용하는 시스템적인 베리언트에 사용하고 
- shader_feature는 선택적으로 사용하는 머터리얼 베리언트로 사용
54. srp batcher (https://discussions.unity.com/t/srp-batcher-and-gpu-instancing/777746)
- skinned mesh 는 원래 batch가 안됨. 하지만 상하체 분리 된 skinned mesh renderer이지만 본을 공유하는 경우는?
- skinned mesh renderer도 결국엔 하나의 bonematrix를 공유함. 따라서 각각의 바츠를 srp batcher로 묶어서 랜더링 가능 (지금은 된다는 말이 있음)
- drawcall 보다 drawcall 전에 하는 상태를 변경하지 않고 drawcall을 하는게 병렬처리에 유리
- 이유는 gpu 입장에선 drawcall 하나가 다 끝나지 않아도 다음 drawcall을 실행함
- srp batch는 하나의 큰 머터리얼 buffer를 같은 쉐이더를 가진 다른 머터리얼이 공유하고 setpass를 깨지 않는데 의의가 있음
- 그러니깐 예전엔 같은 쉐이더라도 머터리얼 속성이 다르면 setpass를 다시 했음. srp 는 그걸 안함
- 순서라는 속성과 같은 쉐이더는 서로 연관성이 없으니 같은 쉐이더라도 거리가 다르면 다른 쉐이더를 쓰는 거리가 같은 것에 의해 깨질 수 있음 (그래서 불투명에 유리)
55. shader category = character(hair/face/body), none character, terrain, fx
56. normal vs tangent space (https://learnopengl.com/Advanced-Lighting/Normal-Mapping)
- tangent space -> uniform & 많은 노멀 
- E1, E2는 오브젝트 스페이스의 버텍스 좌표간 차이로 구한 3차원 방향 벡터
- E1, E2는 Z 값이 있다. 하지만 텍스쳐 공간에 E1, E2는 2D 공간에 있다?
57. heightmap normal 
- ?
58. true type font
59. xray shader (https://www.youtube.com/watch?v=WmnYhIwWYfU)
60. programming language (https://tech.devsisters.com/posts/programming-languages-1-type-theory)
- int? length = nickname?.Length;
61. client 최적화
- zstring
- effect data/spawn/pooling
- _update (유니티 Update 삭제. custom update)
- prebuild(preassemble)
- concurrent async resource load (based on resource dependency map)
- depth/shadow 그릴 때 lod / shadow caster 타입 분류
- prepass Z (early z) 최적화 (깊이 버퍼 재활용)
62. server 최적화
- batch sql/network
- global memory cache
- distribute system
63. LUT Mapping (https://docs.unity3d.com/Packages/com.unity.sensorsdk@2.0/manual/UserGuide/UseLUTMappingForVisualization.html)
64. Setpass calls vs Drawcalls (https://discussions.unity.com/t/setpass-calls-vs-drawcalls/685754)
- Setpass calls is set material properties (cbuffer)
65. instancing
- 동일한 메쉬 다수를 uniform은 하나에 index 별로 구분하여 한번에 그리는 방법 (마지막 인자 amount)
- 당연히 동일한 쉐이더 사용
- 어자피 루프를 돈다 그럼 차이점은? (개별적으로 bind VAO, bind texture, set uniform etc를 loop 안에서 안해도 된다)
    glBindVertexArray(rock.meshes[i].VAO); // 최소 개별 매쉬의 vao 는 binding 함
    glDrawElementsInstanced(
        GL_TRIANGLES, rock.meshes[i].indices.size(), GL_UNSIGNED_INT, 0, amount
    );
66. type AND vs OR
- OR 합집합. 합한다. 집하에 같은 원소는 없다.
- 이거 아니면 저거로 구조적으로 갈림
- AND 곱집합. 곱한다. 
- 이것과 저것으로 구조적 결합
- type composition
- 2개의 주사위를 던저 합이 3이 나올 확률
- 곱집합-합집합-곱집합
- 곱집합은 조합의 모든 경우 수고 합집합은 그 조합을 더한것
- type(class)은 합의 법칙을 따름 (OR) (type만 봐도 의미를 추론 가능해야 할만큼 명확해야 함)
- has a는 곱의 법칙을 따름 (AND)
- Bloom : PostProcessEffectSettings
- DepthOfField : PostProcessEffectSettings 
- PostProcessEffectRenderer<Bloom> a;
- PostProcessEffectRenderer<DepthOfField> b;
- a와 b는 타입이 다르니 or 관계이고 Bloom과 DepthOfField를 has a 하니 AND 관계 (OR와 AND가 같이 있음)
- 타입은 동작을 정의하고 서로 다른 데이터를 generic화해서 타입이 원하는 데이터를 준다. 
67. 캐릭터 주변 light (ShadeSHPerPixel+light에 추가적으로 거리에 따른 밝기를 줬다)
- #if defined(CHARACTER_LIGHT_ON)
- 	half3 toLight = (_Q5GlowbalCharacterLight.xyz - data.worldPos);
- 	half dist = length(toLight) * _Q5GlowbalCharacterLight.w;
- 	half att = saturate((1.h - dist * dist)*_Q5GlowbalCharacterLightColor.w);
-   o_gi.indirect.diffuse += _Q5GlowbalCharacterLightColor.xyz * att;
- #endif
68. prepass z (https://community.khronos.org/t/early-z-and-discard/74748/5; https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-28-graphics-pipeline-performance)
- default framebuffer (최초 원도우 생성시 graphic api 메모리 어딘가에 생성해서 가지고 있고 0번 index 임)
- off-screen rendering은 default framebuffer가 아닌 따로 유저가 생성한 framebuffer에 그리는걸 말하고 default framebuffer에 그리는게 아니라 화면에 보이지 않음
- 최종적으론 0 번(유니티는 RT0)으로 bind하고 off-screen buffer를 quard rect로 읽어서 다시 그림(X) (blit을 사용. 70)
- earlyz는 prepass에서 0번 depth에 그리고 opaque render에선 depth write를 끄고 rt0 depth를 dont clear하고 그림
- prepass z를 불투명 오브젝트를 그리는데 사용하는 off-screen buffer의 깊이 버퍼에 attach 해서 사용해야 함
69. world space UI (https://docs.unity3d.com/Packages/com.unity.ugui@1.0/manual/HOWTO-UIWorldSpace.html)
70. blit (glBlitFramebuffer)
- 복잡한 그래픽 파이프 라인을 거치지 않는 단순 복사. 하드웨어 가속 지원 매우 빠름
- 영역을 지정하여 복사 가능. 
- MSAA 렌더버퍼 생성: `glRenderbufferStorageMultisample`을 사용하여 멀티샘플 원본 렌더버퍼를 생성
- 블리팅 수행: `glBlitFramebuffer`를 사용하여 MSAA 버퍼에서 일반 텍스처로 블리팅
- 필터링: `GL_NEAREST`를 사용하여 MSAA 샘플을 결합합니다. 이는 MSAA의 안티앨리어싱 효과를 유지
- 기본 프레임버퍼(default framebuffer)도 멀티샘플로 생성 가능
- // 렌더링 루프
- while (!glfwWindowShouldClose(window)) {
-   // 1. MSAA FBO에 렌더링
-   glBindFramebuffer(GL_FRAMEBUFFER, msaaFBO);
-   // 여기서 장면 렌더링
-
-   // 2. MSAA 해상도 및 중간 FBO로 블리팅
-   glBindFramebuffer(GL_READ_FRAMEBUFFER, msaaFBO);
-   glBindFramebuffer(GL_DRAW_FRAMEBUFFER, intermediateFBO);
-   glBlitFramebuffer(0, 0, width, height, 0, 0, width, height, GL_COLOR_BUFFER_BIT, GL_NEAREST);
-
-   // 3. 포스트 프로세싱
-   glBindFramebuffer(GL_FRAMEBUFFER, intermediateFBO);
-   // 여기서 포스트 프로세싱 셰이더 적용
-
-   // 4. 최종 결과를 기본 프레임버퍼로 블리팅
-   glBindFramebuffer(GL_READ_FRAMEBUFFER, intermediateFBO);
-   glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0);
-   glBlitFramebuffer(0, 0, width, height, 0, 0, width, height, GL_COLOR_BUFFER_BIT, GL_NEAREST);
-
-   glfwSwapBuffers(window);
-   glfwPollEvents();
-}
71. unity prepass-job (https://docs.unity3d.com/Manual/SL-CameraDepthTexture.html)
- _cameraMain.depthTextureMode = DepthTextureMode.Depth or DepthNormals or MotionVectors
- prepass에서도 depth에 그리고 opaque에서도 쓴다.
72. shader replacement
73. pcf vs msaa (https://www.reddit.com/r/opengl/comments/9iay3t/benefits_of_using_a_multisampled_depth_texture/)
- msaa는 레스터라이즈 단계에서 한 픽셀의 색을 결정할 때 한 픽셀 영역안의 코드 4개를 셈플링하여 평균을 냄
- pcf는 주변 픽셀을 같이 셈플링하여 평균을 냄
74. 순열과 조합 그리고 벡트래킹(https://uiop5809.tistory.com/345?category=1101320)
75. depth buffer와 depth texture는 다름 (https://discussions.unity.com/t/custom-shader-not-writing-to-depth-buffer/826767/2)
- depth buffer를 통해 culling을 처리
- depth texture은 셈플링 가능하여 image processing에서 사용
76. nCr, nPr
- r개의 for loop가 필요
77. refraction (https://darkcatgame.tistory.com/110)
- 버텍스를 클립 스페이스 좌표로 변환하면 
- 전체 화면(grabpass) 중 버텍스의 동차 좌표상 위치가 되고
- 그 좌표를 w로 나누면 전체 화면 화면상 버텍스의 ndc 좌표가 됨 (grabpass 화면은 원근 투영이여야 함)
- 프레임 버퍼의 color attachement를 읽는 방법
78. mvvm pattern (https://velog.io/@kyeun95/%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8C%A8%ED%84%B4-MVVM-%ED%8C%A8%ED%84%B4%EC%9D%B4%EB%9E%80)
79. 평면 방정식
- 벡터 + dot의 4개의 원소로 정의
80. ComputeGrabScreenPos(o.pos) vs screenUV = i.screenPos.xy / i.screenPos.w
- 결국엔 버텍스 쉐이더에서 xy를 -0.5w ~ 0.5w 범위 결과에 0.5 w를 더함으로서 0~w로 1차 변환해 준 것
- ComputeGrabScreenPos 보다 같은 기능의 ComputeScreenPos를 사용하면 됨
- 그리고 두 함수의 결과는 당연히 w로 나눠야 해서 tex2dProj를 사용해야함
- float4 o = pos * 0.5f; 
- o.xy = float2(o.x, o.y) + o.w; // o.w 는 위에서 0.5를 곱했으니 사실 0.5w 크기임 착각하기 쉬움
81. scene depth, screen position Shader Graph Node convert
- ComputeScreenPos와 같은 원리 픽셀에서 현재 화면 텍스쳐상 uv 찾기임
- 즉 버텍스에서 ComputeScreenPos로 넘겨 받은 값을 tex2dProj 함
- screen position == ComputeScreenPos로 넘겨 받은 값의 픽셀 쉐이더 값 그 자체
- scene depth는 screen position를 입력으로 LinearEyeDepth 한거
82. world positino in pixel shader
- // Vertex Shader
- float4 worldPos = mul(unity_ObjectToWorld, v.vertex);
- o.worldPos = worldPos;
- // Pixel Shader
- float3 worldPos = i.worldPos.xyz; OR
- float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, IN.uv);
- float4 clipSpacePos = float4(IN.screenPos.xy, depth, 1.0);
- float4 viewSpacePos = Mul(UNITY_MATRIX_I_PROJECTION, clipSpacePos);
- viewSpacePos /= viewSpacePos.w;
- float4 worldPos = mul(UNITY_MATRIX_I_V, viewSpacePos);
83. out.positionCS 와 ComputeScreenPos를 사용하여 pixel에서 uv 계산시 다른점은? (https://chatgpt.com/c/5562a38f-db42-4f19-a68c-c88b34ed38e5)
-  Y축 반전 문제
- 픽셀 안에서 w로 나누는게 정밀도가 높다 등.
84. surface shader에선 Pass와 tag로 LightMode 지정 불가
- surf Lambert 처럼 light는 template 지정하면 알아서 LightMode 생성
85. shader toy (https://www.shadertoy.com/view/Xl2XRW)
- water noise distortion (https://www.shadertoy.com/view/lfy3Rz)
- simple sin wave (https://www.shadertoy.com/view/MsX3DN)
- https://www.shadertoy.com/view/fllXD4
- https://www.shadertoy.com/view/lsBSWm
86. transparency overlaps itself
- https://gamedev.stackexchange.com/questions/72621/self-occluding-object-and-alpha-blending
- front face이지만 카메라 방향과 반대이면 backface와 마찬가지로 컬링됨
- FXAA, TAA PP 방식으로 알파 테스트의 단점을 개선할 수 있긴 함
87. noise (일관성이 있냐? 없냐?)
- https://www.youtube.com/watch?v=YcdldZ1E9gU&list=PLRqwX-V7Uu6bgPNQAdxQZpJuJCjeOr7VD&index=2
- https://discussions.unity.com/t/how-to-convert-gradient-noise-into-a-texture-in-shader-graph/861715
- https://www.youtube.com/shorts/vZxVANlCbt4
88. rendering equation
- https://learnopengl.com/PBR/Theory
- 자신의 Radiance를 구할 때 자신으로 들어오는 다른 모든 Radiance를 입력으로 받아 재귀적으로 계산
- https://pjessesco.tistory.com/49
89. water wave (https://youtu.be/Ic_3p90NdxM?si=S_qOqPGQeWiA0BX)
90. everything effect (https://www.youtube.com/watch?v=Uc8FEI5bg3w)
91. fresnel 값을 통한 refraction과 reflection 비율 나누기 enery conservation
92. pbr ibl vs lightmap
- prb ibl은 동적인 light, lihgtmap은 정적인 light. 즉 아예 서로 달라서 간섭이 없음
- 다시 말해 정적 light는 동적으로 존재하지 않음 
93. light color를 입력으로 specular+diffuse등을 계산해야 함
- matalic은 diffuse map을 어떻게?
94. TextureFormat.RGBAHalf + hdr + pbr
- 8 비트 채널 -> 16 비트 채널로 hdr 표현 그리고 개별 light intensity는 제어 개별 불가 하니 pbr 에선 hdr
95. Cook-Torrance
- https://blog.naver.com/canny708/221551990444
- 비등방성(anisotropic) 빛을 받는 방향에 따라 반사율이 달리 적용 되는 성질
96. dot(halfway, view)
- dot은 두 벡터간 정렬 상태를 수치로 표현함
-view와 light를 내적하여 둘간의 관계 즉. 같은 곳을 바라보는지 같이 직접적으로 내적하지 않는 이유는 90 이상 벌어졌을 시 음수가 나오기 때문
- halfway 와 내적하면 90 도 이상 벌어져도 양수 실수로 수치를 가질 수 있고 빛과 시선은 180 이상은 지면아래라 의미 없으니 0~180 사이의 값만 의미 있음
- 따라서 halfway와 view 내적이 0-180 간 수치를 내기 딱 좋은 조건임
97. boat trail/ship wave/water wake (Kelvin wake pattern)
- https://www.youtube.com/watch?v=95sQcSulRFM
- https://discourse.mcneel.com/t/water-shape-made-by-boats-when-moving/184789/15
- https://www.shadertoy.com/view/4llBRl
- https://www.youtube.com/watch?v=byE47MX8gtA
- https://realtimevfx.com/t/realtime-water-and-ship-fx-in-ue5-3/25524/2
- https://discourse.mcneel.com/t/ocean-wave-generation-in-rhino/75223/28
- https://www.youtube.com/watch?v=fJpgEvQyRt4&list=PL9vAaGGYE8Tp7NtUdpZbTlEjypWLs4H2M&index=5
- https://www.youtube.com/watch?v=V4yZigMSLiU&t=208s (gerstner waves)
98. PBR IBL vs GI
- https://lifeisforu.tistory.com/375
- IBL 은 하나의 커다란 광원
99. wave
- wave는 물리적으로 시간당 이동 거리에 따른 높이의 변화량을 라디안(각도)의 
- 변화를 통해 서술하고, 
- 이러한 각도의 변화는 사인파 등을 사용하여 높이로 변환 할 수 있다.
- 결국 이동 거리에 따른 높이의 변화를 주기성을 가지는 각도로 서술한다 이다.
- 한편, 파수(wavenumber)등의 개념을 이용해 wave의 스케일링 등을 확장할 수 있고
- 사인파 대산 커스텀 그래프, 노이즈 적용 등을 통하여 다양한 Wave 표현할 수 있다.
- displacement (https://www.shadertoy.com/view/4ldGDB)
100. 4k
101. water depth
- 물 깊이를 카메라에 의존적이지 않게 계산하려면 픽셀 쉐이더에서 픽셀의 좌표를 inverseProjectionView
- 하여 월드로 변환 후 물 표면과 물 바닥도 같은 방법으로 가져와 projection을 빼내고 view 위치도 
- 빼내서 순수 월드 좌표로만 확인 해야 함
102. unreal5 pix on windows (https://techartnomad.tistory.com/223)
103. 풀 넘어뜨리기 (https://www.youtube.com/watch?v=nDfK66kj6XA&ab_channel=UnrealEngineKR)
104. RDG (https://techartnomad.tistory.com/204)
105. compute shader
- gpu가 상태를 메모리에 관리 안하는 이유가 병렬 처리시 공유 문제가 있기 때문. 
- 그래서 정 필요하면 텍스쳐를 사용함
- 하지만 compute shader는 thread groups간 메모리 공유가 가능함! (https://stackoverflow.com/questions/44181875/ray-tracing-via-compute-shader-vs-screen-quad)
106. Energy conservation 관련해서 
- 그러니깐 BRDF는 반사에 관련 된 것만이고 
- In physically based rendering however, we make the simplifying assumption that all refracted light gets absorbed and scattered at a very small area of impact, ignoring the effect of scattered light rays that would've exited the surface at a distance.
- 왜냐하면 재질에 흡수 되거나 그 재질 안에서 다시 흩어져 밖으로 나와 다시 보이는 양 자체는 매우 작다고 가정한다는 거네
- 또한 금속성은 모두 흡수 또는 반사만 존재한다이고...
- **PBR**에서는 재질 내부에서 투과된 빛이 산란되는 과정을 생략하고, 투과된 빛이 모두 흡수된다고 가정합니다.
- **금속성 재질**은 **반사와 흡수**만 존재하고, **비금속성 재질**은 **반사와 난반사** 모두를 다룹니다.
- 단! Metallic 1과 Roughness 0 상태는 그러니깐 사실 빛의 반사 특성일 뿐 거울처럼 반사 된 이미지는 절대로 안보이겠네
하지만 pbr ibr 이나 rumen의 경우 gi ambient로 rgb를 가져오니 해상도에 따라 반사가 흐릿하겐 보이겠어



연속적인 느낌

파티클로는 잘 안사는 느낌

sea wave

https://www.desmos.com/calculator/vjwsd3z7ld
https://chatgpt.com/c/67146599-05b4-8008-a442-18fbd237525b

d\ \cdot\left(1-e^{-s\cdot x}\right)

qanda

1. unreal 5 material diff against previous version
2. 소스 빌드 시 content example project는 어디서 가져옴?
3. 나나이트와 lumen workflow 조사
4. compute shader (kd tree로 분할 처리 등)
5. c++ 코드가 부모인 blueprint
6. metalic and roughness and reflection
만약 metallic 1 이고 roughness가 0이라고 치자
그럼 해당 오브젝트는 거울처럼 동작할까?
아니다. 난반사가 없고 금속성이라 투과가 없으니 albedo와 각도에 따른 
빛의 쌔기가 그대로 전해질 뿐이지 
현실 물리처럼 거울처럼 보이진 않을거다.
그래서 완전 반사를 위한 reflection texture를 따로 랜더링해서 
오브젝트 그릴때 반사 이미지를 보여주지 않을까?
7. 아마 데이터 씨드만 코드에서 공유하고 잡은 높낮이 계산 gpu는 실제 버텍스 계산
8. 재질별 큐브를 올려 놓고 루맨 동작을 확인해 봐야겠네
 **Roughness**와 **Metallic**이 모두 1일 때는 각도에 따른 강한 스페큘러 반사가 일어나지 않고, 넓게 분산된 부드러운 반사가 일어나며, 빛의 색상과 혼합된 금속성 반사가 여전히 존재할 수 있지만, 그 반응이 매우 부드럽고 퍼진 형태로 나타나게 됩니다. 둘다 1로 하면 잘 먹는지 눈에 잘 뛸듯

랜러링 방정식에서 모든 직접 광에 for loop를 돌아 계산하고 
추가로 현재 픽셀의 노멀 방향의 PBR IBL 을 가져와 GI ambient를 합산



r.D3D12.AutoAttachPIX=1
r.ShaderDevelopmentMode=1
r.Shaders.Optimize=0
r.Shaders.Symbols=1
r.Shaders.ExtraData=1

풀 / 블랜드 or disolve / 풀 밀기 / unity dither node

1. 물 색은 uv distortion에서 제외
2. 카메라 방향에서 제일 가까운 node
3. 물 색 차이는 최소 값 존재하도록 작업

reflection 강도와 외곡 홍염으로 설명

도대체 머가 90도를 넘어가서 음수가 되지? 어떤 축이? v와 n? l? 그거 말고 있어?

  
BRDF는 랜더링 방정식의 한 부분으로 재질에 따른 입사하는 빛의 기여도를

계산하는 함수이다.

알려진 BRDF 중 microfacet 이론과 물리 법칙을 따르는 BRDF는 높은 

수준의 랜더링 퀄리티를 보장하는데 그 중 unreal 4에서 처음 사용한

다양한 물리 기반 BRDF가 있다. 

이 중 가장 많이 사용 되는 BRDF 인 Cook-Torrance BRDF 에서

Normal distribution function을 가져와 물 표면의 작은 파동에 대한

빛 반사를 표현 하였다.

NDF는 재질을 설명할 때 roughness(거칠기)를 사용하는데

거칠수록 빛이 넓고 고르게 퍼지고 그 반대의 경우 빛의 입사각(+시야각/2)이 표면과 직각이 

될 수록 빛이 집중 된다.

다음의 그래프를 통해 roughness에 따라 빛 입사각(x축)에 때한 빈사(y축)량을 확인 할 수 있다.


r^2
/3.141592(x^2*(r^2-1)+1)^2



제질은 roughness (거칠기) 로 설명하는데 거칠수록 


roughness 표면이 얼마나 거친지 설정하는 값으로 거칠수록 

https://stackoverflow.com/questions/75360978/i-need-help-understanding-the-ggx-normal-distribution-function

동적 오프젝트는 물 위에 있는 걷거나 서있는 넘들만 랜더링 public void OnWillRenderObject()
LOD
투명 삭제
90%


          // float2 noiseXY = tex2D(_NoiseTex, uv2).xy;
          // noiseXY = noiseXY * 2.0 - 1.0;
          // normal2 = normalize(half3(noiseXY.x, noiseXY.y, 1.0));


// 샘플링한 값
float2 noiseXY = tex2D(_GradientNoiseTexture, uv).xy;

// [0, 1] -> [-1, 1]로 변환
noiseXY = noiseXY * 2.0 - 1.0;

// 노멀의 x, y 값에 적용
half3 normal = normalize(half3(noiseXY.x, noiseXY.y, 1.0));



        void surf(Input IN, inout SurfaceOutputStandard o)
        {
            // Sample the base texture
            fixed4 c = tex2D(_MainTex, IN.uv_MainTex);
            o.Albedo = c.rgb;

            // Sample the normal map
            half3 normalTex = UnpackNormal(tex2D(_NormalMap, IN.uv_NormalMap));

            // Generate gradient noise based normal
            half3 noiseNormal = GradientNoise(IN.uv_MainTex);

            // Blend the normal map and noise normal
            half3 blendedNormal = normalize(lerp(normalTex, noiseNormal, _NormalStrength));

            // Assign the blended normal to the output normal
            o.Normal = blendedNormal;
        }



노이즈 스케일

using UnityEditor;
using UnityEngine;

public class GradientNoiseTextureGenerator : EditorWindow
{
    public int textureSize = 256; 
    public float noiseScale = 10.0f; 
    public string savePath = "Assets/GeneratedNoiseTexture.png";

    [MenuItem("Tools/Generate Gradient Noise Texture")]
    public static void ShowWindow()
    {
        EditorWindow.GetWindow(typeof(GradientNoiseTextureGenerator), false, "Noise Texture Generator");
    }

    void OnGUI()
    {
        GUILayout.Label("Noise Texture Settings", EditorStyles.boldLabel);

        textureSize = EditorGUILayout.IntSlider("Texture Size", textureSize, 64, 1024);
        noiseScale = EditorGUILayout.Slider("Noise Scale", noiseScale, 1.0f, 100.0f);

        savePath = EditorGUILayout.TextField("Save Path", savePath);

        if (GUILayout.Button("Generate Noise Texture"))
        {
            Texture2D noiseTexture = GenerateGradientNoiseTexture(textureSize, textureSize, new Vector2(noiseScale, noiseScale));
            SaveTextureAsPNG(noiseTexture, savePath);
            AssetDatabase.Refresh();
            Debug.Log("Noise texture saved to " + savePath);
        }
    }

    void SaveTextureAsPNG(Texture2D texture, string path)
    {
        byte[] bytes = texture.EncodeToPNG();
        System.IO.File.WriteAllBytes(path, bytes);
    }

    Texture2D GenerateGradientNoiseTexture(int width, int height, Vector2 scale)
    {
        Texture2D texture = new Texture2D(width, height, TextureFormat.RGB24, false, true);

        for (int y = 0; y < height; y++)
        {
            for (int x = 0; x < width; x++)
            {
                Vector2 uv = new Vector2((float)x / width, (float)y / height);
                float noiseValue = GenerateNoiseValue(uv, scale);
                texture.SetPixel(x, y, new Color(noiseValue, noiseValue, noiseValue));
            }
        }

        texture.Apply();
        return texture;
    }

    float GenerateNoiseValue(Vector2 uv, Vector2 scale)
    {
        uv *= scale;

        Vector2 ip = new Vector2(Mathf.Floor(uv.x), Mathf.Floor(uv.y));
        Vector2 fp = new Vector2(uv.x - ip.x, uv.y - ip.y);

        float d00 = Vector2.Dot(GradientNoiseDirection(ip), fp);
        float d01 = Vector2.Dot(GradientNoiseDirection(ip + Vector2.up), fp - Vector2.up);
        float d10 = Vector2.Dot(GradientNoiseDirection(ip + Vector2.right), fp - Vector2.right);
        float d11 = Vector2.Dot(GradientNoiseDirection(ip + Vector2.one), fp - Vector2.one);

        fp = fp * fp * fp * (fp * (fp * 6f - new Vector2(15f, 15f)) + new Vector2(10f, 10f));

        float result = Mathf.Lerp(Mathf.Lerp(d00, d01, fp.y), Mathf.Lerp(d10, d11, fp.y), fp.x);
        return result + 0.5f; 
    }

    Vector2 GradientNoiseDirection(Vector2 p)
    {
        float x;
        Hash_LegacyMod_2_1(p, out x);
        return new Vector2(x - Mathf.Floor(x + 0.5f), Mathf.Abs(x) - 0.5f).normalized;
    }

    void Hash_LegacyMod_2_1(Vector2 p, out float x)
    {
        x = Mathf.Sin(Vector2.Dot(p, new Vector2(12.9898f, 78.233f))) * 43758.5453f;
        x = x - Mathf.Floor(x); 
    }
}








float3 FresnelSchlick(float cosTheta, float3 F0)
{
    return F0 + (1.0 - F0) * pow(1.0 - cosTheta, 5.0);
}

float DistributionGGX(float3 N, float3 H, float roughness)
{
    float a = roughness * roughness;
    float a2 = a * a;
    float NdotH = max(dot(N, H), 0.0);
    float NdotH2 = NdotH * NdotH;

    float num = a2;
    float denom = (NdotH2 * (a2 - 1.0) + 1.0);
    denom = PI * denom * denom;

    return num / denom;
}

float GeometrySchlickGGX(float NdotV, float roughness)
{
    float r = (roughness + 1.0);
    float k = (r * r) / 8.0;

    float num = NdotV;
    float denom = NdotV * (1.0 - k) + k;

    return num / denom;
}

float GeometrySmith(float3 N, float3 V, float3 L, float roughness)
{
    float NdotV = max(dot(N, V), 0.0);
    float NdotL = max(dot(N, L), 0.0);
    float ggx2 = GeometrySchlickGGX(NdotV, roughness);
    float ggx1 = GeometrySchlickGGX(NdotL, roughness);

    return ggx1 * ggx2;
}

float3 SpecularCookTorrance(float3 N, float3 V, float3 L, float3 F0, float roughness)
{
    float3 H = normalize(V + L);
    float NDF = DistributionGGX(N, H, roughness); // Normal Distribution Function
    float G   = GeometrySmith(N, V, L, roughness); // Geometry function
    float3 F  = FresnelSchlick(max(dot(H, V), 0.0), F0); // Fresnel equation

    float denominator = 4.0 * max(dot(N, V), 0.0) * max(dot(N, L), 0.0) + 0.001; // Prevent divide by 0
    float3 specular = (NDF * G * F) / denominator;

    return specular;
}








float LightingSpecular(float3 L, float3 N, float3 V, float smoothness)
{
    float3 H = SafeNormalize(float3(L) + float3(V));
    float NdotH = saturate(dot(N, H));
    return pow(NdotH, smoothness);
}

void MainLighting_float(float3 normalWS, float3 positionWS, float3 viewWS, float smoothness, out float specular)
{
    specular = 0.0;

    #ifndef SHADERGRAPH_PREVIEW
    smoothness = exp2(10 * smoothness + 1);
        
    normalWS = normalize(normalWS);
    viewWS = SafeNormalize(viewWS);

    Light mainLight = GetMainLight(TransformWorldToShadowCoord(positionWS));
    specular = LightingSpecular(mainLight.direction, normalWS, viewWS, smoothness);
    #endif
}

float CameraRelativeDepthFade(float4 screenPosition, float depthFadeDistance)
{
    // Scene Depth: 현재 스크린 좌표에서의 뎁스 값을 가져옵니다.
    float sceneDepth = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE_PROJ(_CameraDepthTexture, UNITY_PROJ_COORD(screenPosition)));

    // 현재 카메라 공간에서의 깊이와 장면 깊이의 차이 계산
    float depthDifference = sceneDepth - screenPosition.z;

    // 깊이 차이를 depthFadeDistance로 나누어 0~1로 정규화
    float fadeFactor = saturate(depthDifference / depthFadeDistance);

    // 페이드 효과를 위해 1에서 fadeFactor를 뺍니다.
    return 1.0 - fadeFactor;
}



ShadeSHPerPixel 와 lightmap은 배타적 관계?

ShadeSHPerPixel과 lightmap은 완전히 배타적인 관계는 아닙니다. 두 기술은 서로 다른 목적으로 사용되며, 때로는 함께 사용될 수 있습니다:

1. ShadeSHPerPixel:
   - 실시간 전역 조명(Global Illumination)을 근사하는 기술입니다.
   - 주변 광(ambient light)을 더 정확하게 계산하여 장면에 자연스러운 조명을 제공합니다.
   - 동적인 객체나 환경에 적합합니다.

2. Lightmap:
   - 미리 계산된 조명 정보를 텍스처에 저장하는 기술입니다.
   - 정적인 환경에서 고품질의 조명 효과를 제공합니다.
   - 실행 시 계산 비용이 낮지만, 동적인 조명 변화에는 적합하지 않습니다.

이 두 기술은 다음과 같이 함께 사용될 수 있습니다:

1. 정적 요소에는 lightmap을 사용하고, 동적 요소에는 ShadeSHPerPixel을 적용할 수 있습니다.
2. Lightmap으로 기본 조명을 제공하고, ShadeSHPerPixel로 추가적인 환경 조명 효과를 더할 수 있습니다.

따라서 ShadeSHPerPixel과 lightmap은 상호 보완적으로 사용될 수 있으며, 각각의 장점을 활용하여 더 나은 조명 품질을 얻을 수 있습니다.


ShadeSHPerPixel과 reflection probe는 비슷한 목적을 가지고 있지만, 서로 다른 기술입니다. 이 두 가지를 비교해보겠습니다:

1. ShadeSHPerPixel:
   - 주로 diffuse 전역 조명(Global Illumination)을 근사하는 데 사용됩니다.
   - 구면 조화 함수(Spherical Harmonics)를 사용하여 주변 조명을 계산합니다.
   - 주로 부드러운 조명 효과를 표현하는 데 적합합니다.
   - 계산 비용이 상대적으로 낮습니다.

2. Reflection Probe:
   - 주로 specular 반사를 위한 환경 맵을 제공합니다.
   - 큐브맵이나 구면 파노라마 텍스처를 사용하여 주변 환경을 캡처합니다.
   - 거울같은 표면이나 광택 있는 물체의 반사를 표현하는 데 사용됩니다.
   - 더 세밀한 반사 정보를 제공할 수 있지만, 메모리 사용량이 더 큽니다.

주요 차이점:
- ShadeSHPerPixel은 주로 부드러운 확산 조명을 위한 것이고, reflection probe는 鏡面 반사를 위한 것입니다.
- ShadeSHPerPixel은 더 적은 데이터로 전체적인 조명을 근사하지만, reflection probe는 더 상세한 환경 정보를 제공합니다.

실제 렌더링 파이프라인에서는 이 두 기술을 함께 사용하여 더 realistic한 조명 효과를 얻을 수 있습니다:
- ShadeSHPerPixel로 전체적인 주변 조명을 계산하고
- Reflection probe로 반사 효과를 추가하는 방식입니다.

이렇게 사용하면 diffuse와 specular 조명 모두를 효과적으로 처리할 수 있습니다











Gradient Noise 노드와 비슷한 효과를 가지면서 성능이 더 나은 HLSL 코드를 작성해보겠습니다. Gradient Noise는 주로 퍼린 노이즈(Perlin Noise)를 생성하는 데 사용되며, 주로 텍스처를 만드는 데 사용됩니다. 최적화된 버전은 유사한 결과를 생성하지만, 연산 비용이 적게 들도록 간소화된 노이즈를 사용해야 합니다.

아래는 HLSL로 작성된 간소화된 노이즈 함수입니다:

```hlsl
float Hash21(float2 p) {
    p = frac(p * float2(443.8975, 397.2973));
    p += dot(p, p + 23.7);
    return frac((p.x + p.y) * p.x);
}

float SimpleNoise(float2 uv) {
    float2 i = floor(uv);
    float2 f = frac(uv);

    float a = Hash21(i);
    float b = Hash21(i + float2(1.0, 0.0));
    float c = Hash21(i + float2(0.0, 1.0));
    float d = Hash21(i + float2(1.0, 1.0));

    float2 u = f * f * (3.0 - 2.0 * f);

    return lerp(a, b, u.x) + (c - a) * u.y * (1.0 - u.x) + (d - b) * u.x * u.y;
}

float4 main(float2 uv : TEXCOORD) : SV_Target {
    float noiseValue = SimpleNoise(uv * 10.0); // 스케일 값을 조정하여 노이즈의 주기를 조절합니다.
    return float4(noiseValue, noiseValue, noiseValue, 1.0);
}
```

이 함수는 간단한 해시 함수 `Hash21`를 사용하여 퍼린 노이즈와 유사한 결과를 생성합니다. 계산 비용을 줄이기 위해 간단한 보간 방법을 사용했습니다. 이 함수는 "퍼린 노이즈"와 매우 유사한 결과를 얻을 수 있지만, 성능상 이점이 있습니다. 

특정한 효과나 필요한 비주얼에 맞추어 스케일 값이나 보간 방법을 조정할 수 있습니다. 이 코드가 GPU 상에서 더 빠르게 동작하도록 최적화되었습니다.







trigger, layer, volume, Profile <-> framebuffer 

quick cpu
https://coderbag.com/product/quickcpu

https://docs.unity3d.com/Manual/LinearRendering-LinearOrGammaWorkflow.html
https://forum.unity.com/threads/question-about-gamma-and-linear-work-flow-in-unity.1147007/



https://panoskarabelas.com/posts/screen_space_shadows/
https://forum.unity.com/threads/what-are-screen-space-shadows.1363755/

https://www.geeksforgeeks.org/problems/median-in-a-row-wise-sorted-matrix1527/0

https://bytewrangler.blogspot.com/2011/10/signed-distance-fields.html
https://linkmemo.tistory.com/category/C%2B%2B%20%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D
https://stackoverflow.com/questions/2321511/what-is-meant-by-resource-acquisition-is-initialization-raii


https://stackoverflow.com/questions/20895648/difference-in-make-shared-and-normal-shared-ptr-in-c




	fixed4 FragBloom(VaryingsDefault i) : SV_Target
	{
		half4 color = SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, i.texcoord);
		return color;
		
		//color.rgb = tonemap_uc2(color.rgb);

		//  // reinhard tone mapping
		//  half3 mapped = color / (color + half3(1.0, 1.0, 1.0));
		//  // gamma correction 
		//  mapped = pow(mapped, half(1.0) / 2.2);
		//  color = half4(mapped, color.a);	
		// return color;

		half3 bloom = SAMPLE_TEXTURE2D(_Bloom_Result, sampler_Bloom_Result, i.texcoord).rgb *
			ONE_MINUS_THRESHHOLD_TIMES_INTENSITY;
		color.rgb += bloom;

		#if UNITY_COLORSPACE_GAMMA
			color.rgb = clamp(color.rgb, 0, 4);
			color.rgb = GammaToLinearSpace(color.rgb);
		#endif

		color.rgb = tonemap_uc2(color.rgb);
		color.rgb = (color.rgb - 0.5h) * CONTRAST + 0.5h;

		color.rgb *= _RemapColor.rgb;

		fixed lum = Luminance(color.rgb);
		color.rgb = lerp(fixed3(lum, lum, lum), color.rgb, SATURATION);

		//#if UNITY_COLORSPACE_GAMMA
		//color.rgb = LinearToGammaSpace(color.rgb);;
		//#endif

		return color;
	}



  네, 물론입니다. 각 속성에 대한 코드 예시를 들어 설명해 드리겠습니다.

1. Metallic (금속성):
```glsl
float metallic = _Metallic * texMask.g;
```
이 코드는 금속성을 계산합니다. _Metallic은 전체적인 금속성 값이고, texMask.g (녹색 채널)로 세부 조정합니다.

예시 사용:
```glsl
// 반사 계산
float3 reflection = CalculateReflection(viewDirection, normal);
float3 finalColor = lerp(albedo, reflection, metallic);
```

2. Smoothness (매끄러움):
```glsl
float smoothness = _Glossiness;
```
매끄러움은 _Glossiness 값으로 직접 설정됩니다.

예시 사용:
```glsl
// 스페큘러 하이라이트 계산
float specPower = exp2(smoothness * 11.0) + 2.0;
float3 specular = CalculateSpecular(lightDir, viewDir, normal, specPower);
```

3. Glossiness (광택):
```glsl
float glossiness = texMask.r;
```
광택은 텍스처 마스크의 빨간 채널에서 가져옵니다.

예시 사용:
```glsl
// 프레넬 효과 계산
float fresnel = pow(1.0 - saturate(dot(normal, viewDir)), 5.0) * glossiness;
finalColor += fresnel * specularColor;
```

4. Emission (방출):
```glsl
float3 emission = _Emissive.rgb * texMask.b;
```
방출 색상은 _Emissive 색상과 텍스처 마스크의 파란 채널을 곱하여 계산합니다.

예시 사용:
```glsl
// 최종 색상에 방출광 추가
finalColor += emission;
```

이러한 속성들을 조합하여 사용하는 예시:

```glsl
void surf (Input IN, inout SurfaceOutputStandard o)
{
    // 기본 텍스처와 속성들 샘플링
    fixed4 c = tex2D (_MainTex, IN.uv_MainTex);
    fixed4 texMask = tex2D (_MaskTex, IN.uv_MaskTex);
    
    o.Albedo = c.rgb;
    o.Metallic = _Metallic * texMask.g;
    o.Smoothness = _Glossiness;
    o.Emission = _Emissive.rgb * texMask.b;
    
    // 광택을 사용한 추가적인 스페큘러 계산
    float glossiness = texMask.r;
    float3 specular = CalculateSpecular(IN.worldNormal, IN.viewDir, _LightColor0.rgb, glossiness);
    
    // 최종 색상 조합
    o.Albedo = lerp(o.Albedo, specular, o.Metallic);
    o.Albedo += o.Emission;
}
```

이 예시에서는 각 속성이 어떻게 서로 상호작용하여 최종 표면 외관에 영향을 미치는지 볼 수 있습니다. 실제 구현은 사용하는 렌더링 파이프라인과 쉐이더 모델에 따라 다를 수 있습니다.

https://claude.ai/chat/04e74493-da9f-4a17-8bc7-1b7c8c03fed8

Substance Painter





텍스쳐 좌표계의 TB 평면의 TB를 

선형 방정식을 이용해 TB를 찾음

입력 값은 오브젝트 좌표계의 좌표를 사용함으로서

오브젝트 좌표계 상 버텍스를 텍스쳐 좌표계로 사실상

맵핑 시킴

그러니깐 선형 방정식의 이용해 T B 벡터를 찾는데 이때 마치 uv 좌표계의 edge 를 실제 텍스쳐 좌표계상 edge 가 아닌 오브젝트 좌표계상 edge 로 구해서 오브젝트 상 버텍스를 텍스쳐 좌표계상에 있는 것처럼 맵핑 시켜서 구하고 있음 

이게 수학의 경의로움 같음

## TBN matrics

TBN 행렬은 텍스쳐 공간(tangent space)에서 모델 공간으로의 변환 

및 역변환을 다룬다는 점에서 서로 다른 공간 간 변환을 다루는 MVP 

행렬과 같은 목적성을 가진다.

<!-- begin_excerpt -->

하지만 TBN 행렬을 사용한 공간 변환은

MVP 행렬을 사용한 변환처럼 기하학적으로 직관적이지 않다.

<!-- end_excerpt -->

이는 모델 공간상 정의 된 버텍스 노멀을 텍스쳐 공간으로

균일한 방법을 통해 인코딩하고 다시 디코딩하는 수학적 수식이라서

추상적이다.

> <font size="2"> 
> TBN 행렬도 기하학적으로 표현할 수 있긴 하지만 정확히 표현하기 어렵다. <br>
> 오히려 수식의 의도를 먼저 이해하고 머릿속으로 기하학적 모양을 상상하는 것이 더 낫다.
> </font>



Post Processing Stack 2를 사용하여 "Before Transparent" 시점에서 Command Buffer로 화면을 캡처한 후, 그 데이터를 다른 패스에서 읽어오는 방법을 설명하겠습니다. 이 방법을 통해 특정 시점의 화면 데이터를 사용해 다양한 효과를 만들 수 있습니다.

### 1. **Command Buffer 설정 및 화면 캡처**

먼저, Command Buffer를 사용하여 `BeforeTransparent` 이벤트 시점에 화면을 캡처해야 합니다.

```csharp
using UnityEngine;
using UnityEngine.Rendering;
using UnityEngine.Rendering.PostProcessing;

public class CaptureSceneColor : MonoBehaviour
{
    private CommandBuffer _commandBuffer;
    private RenderTexture _sceneColorRT;
    private int _sceneColorID;

    void OnEnable()
    {
        _sceneColorID = Shader.PropertyToID("_SceneColorTex");

        int downsampleFactor = 2; // 예: 2배 다운 샘플링

        // RenderTexture 설정
        _sceneColorRT = new RenderTexture(Screen.width/downsampleFactor, Screen.height/downsampleFactor, 0, RenderTextureFormat.Default);
        _sceneColorRT.Create();

        // Command Buffer 생성 및 설정
        _commandBuffer = new CommandBuffer { name = "Capture Scene Color" };
        _commandBuffer.Blit(BuiltinRenderTextureType.CurrentActive, _sceneColorRT);

        // CameraEvent.BeforeTransparent 시점에 Command Buffer 실행
        Camera.main.AddCommandBuffer(CameraEvent.BeforeTransparent, _commandBuffer);

        // 캡처한 텍스처를 쉐이더에서 사용할 수 있도록 설정
        Shader.SetGlobalTexture(_sceneColorID, _sceneColorRT);
    }

    void OnDisable()
    {
        // Command Buffer 해제 및 정리
        Camera.main.RemoveCommandBuffer(CameraEvent.BeforeTransparent, _commandBuffer);
        _commandBuffer.Release();
        _sceneColorRT.Release();
    }
}
```

### 2. **쉐이더에서 캡처한 화면 데이터를 사용**

이제 쉐이더에서 `_SceneColorTex`로 접근할 수 있게 되었습니다. 이 텍스처는 "Before Transparent" 시점에 캡처된 화면 데이터를 포함하고 있습니다.

다음은 해당 텍스처를 사용하는 간단한 HLSL 예제입니다.

```hlsl
Shader "Custom/UseSceneColor"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
    }
    SubShader
    {
        Tags { "RenderType"="Opaque" }
        LOD 100

        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"

            sampler2D _MainTex;
            sampler2D _SceneColorTex; 

            struct appdata
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
            };

            struct v2f
            {
                float2 uv : TEXCOORD0;
                float4 pos : SV_POSITION;
            };

            v2f vert (appdata v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.uv = TRANSFORM_TEX(v.uv, _MainTex);
                return o;
            }

            float4 frag (v2f i) : SV_Target
            {
                // 텍스처 샘플링
                float4 texColor = tex2D(_MainTex, i.uv);

                // 캡처된 화면 색상 샘플링
                float4 sceneColor = tex2D(_SceneColorTex, i.uv);

                // 결합된 결과 반환
                return sceneColor * texColor;
            }
            ENDCG
        }
    }
    FallBack "Diffuse"
}
```

### 3. **후처리 효과(Post Process)와의 결합**

위의 방법을 사용하면 `BeforeTransparent` 시점에 화면을 캡처하고, 이 캡처된 데이터를 쉐이더에서 활용할 수 있습니다. 이 데이터를 사용하여 다양한 후처리 효과를 구현할 수 있습니다.

예를 들어, 특정 후처리 효과를 적용하기 전에 화면의 색상을 캡처한 후, 그 후에 이를 사용하여 원하는 연출을 할 수 있습니다. 이 과정은 유연하게 다양한 시나리오에 맞춰 활용할 수 있습니다.

이 방법은 고급 렌더링 기법을 구현하거나 특정 시점의 화면 데이터를 기반으로 특수 효과를 적용할 때 유용하게 사용될 수 있습니다.



알겠습니다. Unity의 예전 **Built-in** 렌더링 파이프라인에서는 `UNITY_SCENE_COLOR`와 같은 함수가 존재하지 않았습니다. 그래서 직접 화면의 색상을 가져오는 코드를 작성해야 합니다.

다음은 Unity의 **Built-in** 렌더링 파이프라인에서 HLSL을 사용하여 화면의 색상을 샘플링하는 방법에 대한 예제입니다.

```hlsl
Shader "Custom/SceneColorShader"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
    }
    SubShader
    {
        Tags { "RenderType"="Opaque" }
        LOD 100

        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"

            sampler2D _MainTex;
            float4 _MainTex_ST;

            struct appdata
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
            };

            struct v2f
            {
                float2 uv : TEXCOORD0;
                float4 pos : SV_POSITION;
            };

            v2f vert (appdata v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.uv = TRANSFORM_TEX(v.uv, _MainTex);
                return o;
            }

            // Scene Color를 가져오는 코드
            float4 frag (v2f i) : SV_Target
            {
                // 화면의 UV 좌표를 계산합니다.
                float2 screenUV = i.pos.xy / i.pos.w;
                screenUV = screenUV * 0.5 + 0.5; // NDC를 [0,1]로 변환

                // Scene Color를 샘플링
                float4 sceneColor = tex2Dproj(_GrabTexture, UNITY_PROJ_COORD(i.pos));

                // 텍스처 색상과 결합
                float4 texColor = tex2D(_MainTex, i.uv);
                return sceneColor * texColor;
            }
            ENDCG
        }
    }
    FallBack "Diffuse"
}
```

### 설명:
- **_GrabTexture**: Unity에서 현재 화면의 색상을 저장하는 특별한 텍스처입니다. 이 텍스처를 통해 화면의 색상을 샘플링할 수 있습니다.
- **tex2Dproj**: 프로젝션된 좌표로 텍스처를 샘플링하는 함수입니다. 이를 사용하여 화면의 색상을 샘플링할 수 있습니다.
- **screenUV**: 화면의 UV 좌표를 NDC (Normalized Device Coordinates)에서 [0,1] 범위로 변환합니다.

이 코드는 현재 화면의 색상을 가져와서 특정 텍스처와 결합하는 예시입니다. 필요에 따라 이 코드를 수정하여 사용하실 수 있습니다.





#ifndef WATER_LIGHTING_INCLUDED
#define WATER_LIGHTING_INCLUDED

#include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl"
#include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl"

#ifdef _SHADOW_SAMPLES_LOW
    #define SHADOW_ITERATIONS 1
    #define SHADOW_VOLUME
#elif _SHADOW_SAMPLES_MEDIUM
    #define SHADOW_ITERATIONS 2
    #define SHADOW_VOLUME
#elif _SHADOW_SAMPLES_HIGH
    #define SHADOW_ITERATIONS 4
    #define SHADOW_VOLUME
#else
    #define SHADOW_ITERATIONS 0
#endif


#ifdef _SSR_SAMPLES_LOW
    #define SSR_ITERATIONS 8
#elif _SSR_SAMPLES_MEDIUM
    #define SSR_ITERATIONS 16
#elif _SSR_SAMPLES_HIGH
    #define SSR_ITERATIONS 32
#else
    #define SSR_ITERATIONS 4
#endif

///////////////////////////////////////////////////////////////////////////////
//                           Reflection Modes                                //
///////////////////////////////////////////////////////////////////////////////

void Reflection_half(half3 reflectVector, float3 positionWS, half perceptualRoughness, half occlusion, float2 normalizedScreenSpaceUV, out half3 output)
{
    output = GlossyEnvironmentReflection(reflectVector, positionWS, perceptualRoughness, occlusion, normalizedScreenSpaceUV);
}

float3 ViewPosFromDepth(float2 positionNDC, float deviceDepth)
{
    float4 positionCS  = ComputeClipSpacePosition(positionNDC, deviceDepth);
    float4 hpositionVS = mul(UNITY_MATRIX_I_P, positionCS);
    return hpositionVS.xyz / hpositionVS.w;
}

float2 ViewSpacePosToUV(float3 pos)
{
    return ComputeNormalizedDeviceCoordinates(pos, UNITY_MATRIX_P);
}

half OutOfBoundsFade(half2 uv)
{
    half2 fade = 0;
    fade.x = saturate(1 - abs(uv.x - 0.5) * 2);
    fade.y = saturate(1 - abs(uv.y - 0.5) * 2);
    return fade.x * fade.y;
}

void Raymarch_half(float3 origin, float3 direction, half steps, half stepSize, half thickness, out half2 sampleUV, out half valid, out half outOfBounds, out half debug)
{
    sampleUV = 0;
    valid = 0;
    outOfBounds = 0;
    debug = 0;

    float3 baseOrigin = origin;
    
    direction *= stepSize;
    const half rcpStepCount = rcp(steps);
    
    [loop]
    for(int i = 0; i < steps; i++)
    {
        debug++;
        //if(valid == 0)
        {
            origin += direction;
            direction *= 1.5;
            sampleUV = ViewSpacePosToUV(origin);

            outOfBounds = OutOfBoundsFade(sampleUV);
            
            //return;
            
            if(!(sampleUV.x > 1 || sampleUV.x < 0 || sampleUV.y > 1 || sampleUV.y < 0))
            {
                float deviceDepth = SampleSceneDepth(sampleUV);
                float3 samplePos = ViewPosFromDepth(sampleUV, deviceDepth);

                if(distance(samplePos.z, origin.z) > length(direction) * thickness) continue;

                
        
                if(samplePos.z > origin.z)
                {
                    valid = 1;
                    return;
                }
                
            } else
            {
                //outOfBounds = OutOfBoundsFade(sampleUV);
                return;
            }
        }
    }
}

struct WaveParams
{
    half2 origin;
    half amplitude;
    half length;
    half speed;
};



void RadialGerstnerWaves_half(float3 worldPos, half time, out half displacement)
{
    const int waveCount = 1;


    //Params should probably be moved to global scope
    
    WaveParams w1 = {
        half2(0, -164),
        1,
        1,
        2
    };

    WaveParams w2 = {
        half2(20, -130),
        0.7,
        1.7,
        4
    };

    WaveParams w3 = {
        half2(-21, -156),
        0.3,
        3,
        3
    };

    WaveParams w4 = {
        half2(-4, -200),
        1.4,
        0.6,
        1
    };

    WaveParams waveParams[waveCount] = {
        w1,
        //w2,
        //w3,
        //w4
    };
    
    displacement = 0;

    half summedAmplitude = 0;

    for(int i = 0; i < waveCount; i++)
    {
        WaveParams params = waveParams[i];


        half2 D = normalize(worldPos.xz - params.origin);
        //D = half2(1, 0);
        half w = 2/params.length;
        half phaseConstant = w * params.speed;

        displacement += sin( dot(D, worldPos.xz));
        
        summedAmplitude += params.amplitude;
    }

    //displacement /= summedAmplitude;
}

/*
half3 SampleReflections(float3 normalWS, float3 positionWS, float3 viewDirectionWS, half2 screenUV)
{
    half3 reflection = 0;
    half2 refOffset = 0;
    
    float2 uv = float2(0, 0);
    half valid = 1;

    float3 positionVS = TransformWorldToView(positionWS);
    float3 normalVS = TransformWorldToViewDir(normalWS);

    float3 positionVSnorm = normalize(positionVS);
    float3 pivot = reflect(positionVS, normalVS);
    half debug;
    RayMarch(positionVS, pivot, uv, valid, debug);
    half3 ssr = SAMPLE_TEXTURE2D(_CameraOpaqueTexture, sampler_ScreenTextures_linear_clamp, uv).rgb;

    half3 backup = CubemapReflection(viewDirectionWS, positionWS, normalWS);
    reflection = lerp(backup, ssr, valid);
    //do backup
    return reflection;
}
*/


#endif // WATER_LIGHTING_INCLUDED


https://docs.unity3d.com/Packages/com.unity.shadergraph@6.9/manual/Reflection-Probe-Node.html










255가지 색상으로 표현한다면 



 할 때 전구의 


